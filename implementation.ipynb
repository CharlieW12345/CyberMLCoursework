{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3132b62",
   "metadata": {},
   "source": [
    "# Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "a8869cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "trainData = pd.read_csv(\"TrainingDataBinary.csv\") # Dataset is loaded in from CSV files\n",
    "testData = pd.read_csv(\"TestingDataBinary.csv\")\n",
    "\n",
    "#Creating DataFrames for training and testing\n",
    "train_Data = pd.DataFrame(data = trainData)\n",
    "test_Data = pd.DataFrame(data = testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "6d540fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5999 entries, 0 to 5998\n",
      "Columns: 129 entries, 70.399324 to 0.15\n",
      "dtypes: float64(113), int64(16)\n",
      "memory usage: 5.9 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Columns: 128 entries, 12.118057 to 0.51\n",
      "dtypes: float64(112), int64(16)\n",
      "memory usage: 99.1 KB\n"
     ]
    }
   ],
   "source": [
    "train_Data.info() \n",
    "test_Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "cd0b9e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = train_Data.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = train_Data.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Split the data into training and testing sets following the 80/20 split rule\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "bcf7f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1 score\n",
    "# Calculated f1 score manually so I make sure it kept to the formula\n",
    "def f1score(prediction):\n",
    "    TP = 0 \n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for true, pred in zip(y_test, prediction): \n",
    "        if(true == 0 and pred == 0):\n",
    "            TP = TP + 1\n",
    "        if(true == 0 and pred == 1): \n",
    "            FN = FN + 1   \n",
    "        if(true == 1 and pred == 0):\n",
    "            FP = FP + 1\n",
    "        if(true == 1 and pred == 1):\n",
    "            TN = TN + 1 \n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    print(\"precision : \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    print(\"TruePositive: \" + str(TP) + \" FalseNegative: \" + str(FN) + \" FalsePositive: \" + str(FP) + \" TrueNegative: \" + str(TN) )\n",
    "    print(\"f1-score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "bdcb1549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8558333333333333\n",
      "0.9289940828402367\n",
      "precision : 0.8023088023088023\n",
      "recall: 0.9391891891891891\n",
      "TruePositive: 556 FalseNegative: 36 FalsePositive: 137 TrueNegative: 471\n",
      "f1-score: 0.8653696498054475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression() # Create an instance of the LogisticRegression classifier \n",
    "clf.fit(X_train,y_train) # Train the classifier on the training data\n",
    "prediction = clf.predict(X_test) # Make predictions on the testing data\n",
    "# Evaluation metrics\n",
    "print(accuracy_score(y_test, prediction)) # Accuracy score is calculated\n",
    "print(precision_score(y_test, prediction)) # Precision score is calculated\n",
    "f1score(prediction) # f1score is calculated\n",
    "# This process is the same for all of the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f22144a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8766666666666667\n",
      "0.9291044776119403\n",
      "precision : 0.8343373493975904\n",
      "recall: 0.9358108108108109\n",
      "TruePositive: 554 FalseNegative: 38 FalsePositive: 110 TrueNegative: 498\n",
      "f1-score: 0.8821656050955415\n"
     ]
    }
   ],
   "source": [
    "#Linear discriminant analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction)) \n",
    "f1score(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "dad44d1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5216666666666666\n",
      "0.84\n",
      "precision : 0.5078260869565218\n",
      "recall: 0.9864864864864865\n",
      "TruePositive: 584 FalseNegative: 8 FalsePositive: 566 TrueNegative: 42\n",
      "f1-score: 0.6704936854190585\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction))   \n",
    "f1score(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "70137d75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8708333333333333\n",
      "0.9415204678362573\n",
      "precision : 0.8180494905385735\n",
      "recall: 0.9493243243243243\n",
      "TruePositive: 562 FalseNegative: 30 FalsePositive: 125 TrueNegative: 483\n",
      "f1-score: 0.8788115715402659\n"
     ]
    }
   ],
   "source": [
    "# C-Support Vector Classification\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear',C=1.9, gamma='auto')\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction))        \n",
    "f1score(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "01caf0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6041666666666666\n",
      "0.67828418230563\n",
      "precision : 0.5707376058041113\n",
      "recall: 0.7972972972972973\n",
      "TruePositive: 472 FalseNegative: 120 FalsePositive: 355 TrueNegative: 253\n",
      "f1-score: 0.6652572233967582\n"
     ]
    }
   ],
   "source": [
    "#  Multi-layer Perceptron classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction))    \n",
    "f1score(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "8a1f4bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9616666666666667\n",
      "0.9652317880794702\n",
      "precision : 0.9580536912751678\n",
      "recall: 0.964527027027027\n",
      "TruePositive: 571 FalseNegative: 21 FalsePositive: 25 TrueNegative: 583\n",
      "f1-score: 0.9612794612794613\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction))  \n",
    "f1score(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "999a1bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9891666666666666\n",
      "0.996661101836394\n",
      "precision : 0.9816971713810316\n",
      "recall: 0.9966216216216216\n",
      "TruePositive: 590 FalseNegative: 2 FalsePositive: 11 TrueNegative: 597\n",
      "f1-score: 0.9891031014249791\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=150) # These were the best hyper-parameters found for random forest\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "pred_p = clf.predict_proba(X_test)[:,1] # The prediction probabilites for each entry in the model are obtained\n",
    "threshold = 0.6 # 0.6 is chosen to produce more false postives than false negatives\n",
    "for i in range(len(pred_p)): # The values are then re-classified given the threshold\n",
    "    if pred_p[i] >= threshold:\n",
    "        prediction[i] = 1\n",
    "    else:\n",
    "        prediction[i] = 0\n",
    "\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction))    \n",
    "f1score(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "15779cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 1 1 2 2 2 1 1 1 1 1 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1\n",
      " 2 1 1 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\miniconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- -0.18182\n",
      "- -0.20197\n",
      "- -100.061349\n",
      "- -100.072809\n",
      "- -100.124375\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- -100.86922\n",
      "- -102.060972\n",
      "- -119.550481\n",
      "- -119.753909\n",
      "- -120.341499\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\willi\\miniconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- -0.18182\n",
      "- -0.20197\n",
      "- -100.061349\n",
      "- -100.072809\n",
      "- -100.124375\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- -100.86922\n",
      "- -102.060972\n",
      "- -119.550481\n",
      "- -119.753909\n",
      "- -120.341499\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Shows the predicted values from the test data, and then performs re-classification on these predictions\n",
    "prediction = clf.predict(test_Data)\n",
    "print(prediction)\n",
    "# Reclassifies the values based on a threshold again\n",
    "pred_p = clf.predict_proba(test_Data)[:,1]\n",
    "threshold = 0.6\n",
    "for i in range(len(pred_p)):\n",
    "    if pred_p[i] >= threshold:\n",
    "        prediction[i] = 1\n",
    "    else:\n",
    "        prediction[i] = 0\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffdcb0",
   "metadata": {},
   "source": [
    "# Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6da85669",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"TrainingDataMulti.csv\")\n",
    "testData = pd.read_csv(\"TestingDataMulti.csv\")\n",
    "\n",
    "#Creating DataFrames for training and testing\n",
    "train_Data = pd.DataFrame(data = trainData)\n",
    "test_Data = pd.DataFrame(data = testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "bf48cd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5999 entries, 0 to 5998\n",
      "Columns: 129 entries, 70.399324 to 0.15\n",
      "dtypes: float64(113), int64(16)\n",
      "memory usage: 5.9 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Columns: 128 entries, -100.141563 to 0.35\n",
      "dtypes: float64(104), int64(24)\n",
      "memory usage: 99.1 KB\n"
     ]
    }
   ],
   "source": [
    "train_Data.info()\n",
    "test_Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d66436b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_Data.iloc[:, :-1]  \n",
    "y = train_Data.iloc[:, -1]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "7385ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7158333333333333\n",
      "0.6660695553941741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6963103500522466"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix # Have to use new method to calculate f1 score as old method only accounts for binary labels\n",
    "clf = LogisticRegression(max_iter=100000) # To allow for convergence max_iter is added\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction, average='macro')) # Macro is used because the prediction is no longer binary\n",
    "f1_score(y_test,prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "d4fdbb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7033333333333334\n",
      "0.6558584810207017\n",
      "precision : 0.8951612903225806\n",
      "recall: 0.9685863874345549\n",
      "TruePositive: 555 FalseNegative: 18 FalsePositive: 65 TrueNegative: 130\n",
      "f1-score: 0.9304274937133278\n"
     ]
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction, average='macro')) \n",
    "f1_score(y_test,prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "2b2df317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5191666666666667\n",
      "0.4990531852906379\n",
      "precision : 0.7025703794369645\n",
      "recall: 0.9982608695652174\n",
      "TruePositive: 574 FalseNegative: 1 FalsePositive: 243 TrueNegative: 12\n",
      "f1-score: 0.824712643678161\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction, average='macro'))   \n",
    "f1_score(y_test,prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "bfba1974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7025\n",
      "0.6592871607296419\n",
      "precision : 0.877643504531722\n",
      "recall: 0.9880952380952381\n",
      "TruePositive: 581 FalseNegative: 7 FalsePositive: 81 TrueNegative: 140\n",
      "f1-score: 0.9296\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear',C=1.9, gamma='auto')\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction, average='macro'))        \n",
    "f1_score(y_test,prediction, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "70cbbc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8975\n",
      "0.8812404062432124\n",
      "precision : 0.9773123909249564\n",
      "recall: 0.9739130434782609\n",
      "TruePositive: 560 FalseNegative: 15 FalsePositive: 13 TrueNegative: 248\n",
      "f1-score: 0.975609756097561\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction, average='macro'))  \n",
    "f1_score(y_test,prediction, average=\"weighted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "77401ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "0.9506305081182642\n",
      "[[582   5   4]\n",
      " [  3 264  17]\n",
      " [  3  16 306]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9600652448694115"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the multi dataset there are:\n",
    "# 3000 , 0's\n",
    "# 1500 , 1's\n",
    "# 1500 , 2's\n",
    "# a class_weight = {0: 0.6667, 1: 1.3333, 2: 1.3333} was used to help convey this to the model,  but doesn't help the accuracy\n",
    "clf = RandomForestClassifier(n_estimators=150)\n",
    "clf.fit(X_train,y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "print(precision_score(y_test, prediction, average='macro'))  \n",
    "print(confusion_matrix(y_test,prediction)) # Produces the 3 x 3 confusion matrix for the multi data\n",
    "f1_score(y_test,prediction, average=\"weighted\")\n",
    "# The predictions did not need to be reclassified as there are a similar amount of false Negatives to False Postives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "3be3fc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\miniconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- -0.18182\n",
      "- -0.20197\n",
      "- -100.061349\n",
      "- -100.072809\n",
      "- -100.124375\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- -100.86922\n",
      "- -102.060972\n",
      "- -119.550481\n",
      "- -119.753909\n",
      "- -120.341499\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 1 1 2 2 2 1 1 1 1 1 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1\n",
      " 2 1 1 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Produces the final prediction results for the test data\n",
    "prediction = clf.predict(test_Data)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
